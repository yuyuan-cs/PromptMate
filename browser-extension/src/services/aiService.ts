// AIæ¨¡å‹å®šä¹‰æ¥å£
export interface AIModel {
  id: string;
  name: string;
  description?: string;
  contextLength?: number;
  category?: 'chat' | 'embedding' | 'vision' | 'code';
}

// AIæœåŠ¡é…ç½®æ¥å£
export interface AIConfig {
  apiKey: string;
  baseUrl: string;
  model: string;
  provider: 'openai' | 'anthropic' | 'gemini' | 'custom';
}

// AIä¼˜åŒ–è¯·æ±‚æ¥å£
export interface AIOptimizeRequest {
  content: string;
  title?: string;
  mode: 'optimize' | 'generate';
}

// AIä¼˜åŒ–å“åº”æ¥å£
export interface AIOptimizeResponse {
  optimizedContent: string;
  explanation?: string;
  suggestions?: string[];
}

// æµå¼å“åº”å›è°ƒæ¥å£
export interface StreamCallback {
  onChunk: (chunk: string) => void;
  onComplete: (fullResponse: string) => void;
  onError: (error: Error) => void;
}

// å„æœåŠ¡å•†æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
export const AI_MODELS: Record<string, AIModel[]> = {
  openai: [
    { id: 'gpt-4o', name: 'GPT-4o', description: 'æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘', contextLength: 128000 },
    { id: 'gpt-4o-mini', name: 'GPT-4o Mini', description: 'è½»é‡ç‰ˆGPT-4oï¼Œé€Ÿåº¦æ›´å¿«æˆæœ¬æ›´ä½', contextLength: 128000 },
    { id: 'gpt-4-turbo', name: 'GPT-4 Turbo', description: 'é«˜æ€§èƒ½GPT-4æ¨¡å‹', contextLength: 128000 },
    { id: 'gpt-3.5-turbo', name: 'GPT-3.5 Turbo', description: 'æ€§ä»·æ¯”ä¼˜é€‰æ¨¡å‹', contextLength: 16385 }
  ],
  anthropic: [
    { id: 'claude-3-5-sonnet-20241022', name: 'Claude 3.5 Sonnet', description: 'æœ€æ–°Claudeæ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›å¼º', contextLength: 200000 },
    { id: 'claude-3-opus-20240229', name: 'Claude 3 Opus', description: 'æœ€å¼ºæ¨ç†èƒ½åŠ›', contextLength: 200000 },
    { id: 'claude-3-sonnet-20240229', name: 'Claude 3 Sonnet', description: 'å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬', contextLength: 200000 }
  ],
  gemini: [
    { id: 'gemini-1.5-pro', name: 'Gemini 1.5 Pro', description: 'æœ€æ–°Geminiæ¨¡å‹ï¼Œæ”¯æŒé•¿ä¸Šä¸‹æ–‡', contextLength: 2000000 },
    { id: 'gemini-1.5-flash', name: 'Gemini 1.5 Flash', description: 'å¿«é€Ÿç‰ˆæœ¬ï¼Œé€‚åˆå®æ—¶åº”ç”¨', contextLength: 1000000 },
    { id: 'gemini-pro', name: 'Gemini Pro', description: 'ç»å…¸Geminiæ¨¡å‹', contextLength: 32768 }
  ],
  custom: [
    { id: 'custom-model', name: 'è‡ªå®šä¹‰æ¨¡å‹', description: 'è¯·é…ç½®æ‚¨çš„è‡ªå®šä¹‰æ¨¡å‹', contextLength: 4096 }
  ]
};

// è·å–æŒ‡å®šæœåŠ¡å•†çš„æ¨¡å‹åˆ—è¡¨
export function getModelsForProvider(provider: string): AIModel[] {
  return AI_MODELS[provider] || [];
}

// ç²¾ç®€çš„æç¤ºè¯ä¼˜åŒ–æ¨¡æ¿
const PROMPT_OPTIMIZATION_TEMPLATE = `ä½œä¸ºæç¤ºè¯ä¸“å®¶ï¼Œä¼˜åŒ–ä»¥ä¸‹å†…å®¹ï¼š

æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content}

è¦æ±‚ï¼š
1. ä¼˜åŒ–æç¤ºè¯ç»“æ„å’Œæ¸…æ™°åº¦
2. ç¡®ä¿åŒ…å«è§’è‰²ã€ä»»åŠ¡ã€æ ¼å¼è¦æ±‚
3. æä¾›ç®€è¦ä¼˜åŒ–è¯´æ˜

æ ¼å¼ï¼š
## ä¼˜åŒ–åçš„æç¤ºè¯
[ä¼˜åŒ–å†…å®¹]

## ä¼˜åŒ–è¯´æ˜
[å…³é”®æ”¹è¿›ç‚¹]

## å»ºè®®
[1-2æ¡å®ç”¨å»ºè®®]`;

// AIæœåŠ¡ç±»
export class AIService {
  private config: AIConfig | null = null;

  constructor() {
    this.loadConfig();
  }

  // åŠ è½½é…ç½®
  private loadConfig() {
    try {
      const savedConfig = localStorage.getItem('ai-config');
      if (savedConfig) {
        this.config = JSON.parse(savedConfig);
      }
    } catch (error) {
      console.error('åŠ è½½AIé…ç½®å¤±è´¥:', error);
    }
  }

  // ä¿å­˜é…ç½®
  public saveConfig(config: AIConfig) {
    this.config = config;
    localStorage.setItem('ai-config', JSON.stringify(config));
  }

  // è·å–é…ç½®
  public getConfig(): AIConfig | null {
    return this.config;
  }

  // æ£€æŸ¥æ˜¯å¦å·²é…ç½®
  public isConfigured(): boolean {
    const configured = this.config !== null && this.config.apiKey.trim() !== '';
    console.log('ğŸ” AIæœåŠ¡é…ç½®æ£€æŸ¥:', { 
      hasConfig: this.config !== null, 
      hasApiKey: this.config?.apiKey?.trim() !== '', 
      configured 
    });
    return configured;
  }

  // æµ‹è¯•è¿æ¥
  public async testConnection(config: AIConfig): Promise<boolean> {
    const { apiKey, baseUrl, model, provider } = config;

    // æ„å»ºæµ‹è¯•è¯·æ±‚
    let requestBody: any;
    let headers: Record<string, string>;

    switch (provider) {
      case 'openai':
        requestBody = {
          model: model || 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: 'Hello, this is a test message.' }],
          max_tokens: 10
        };
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        break;

      case 'anthropic':
        requestBody = {
          model: model || 'claude-3-sonnet-20240229',
          max_tokens: 10,
          messages: [{ role: 'user', content: 'Hello, this is a test message.' }]
        };
        headers = {
          'Content-Type': 'application/json',
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01'
        };
        break;

      case 'gemini':
        requestBody = {
          contents: [{ role: 'user', parts: [{ text: 'Hello, this is a test message.' }] }],
          generationConfig: { maxOutputTokens: 10 }
        };
        headers = { 'Content-Type': 'application/json' };
        break;

      case 'custom':
      default:
        requestBody = {
          model: model,
          messages: [{ role: 'user', content: 'Hello, this is a test message.' }],
          max_tokens: 10
        };
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        break;
    }

    try {
      const requestUrl = this.buildRequestUrl(provider, baseUrl, model, apiKey);
      
      const response = await fetch(requestUrl, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody)
      });

      if (!response.ok) {
        return false;
      }

      const data = await response.json();
      
      // æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«é¢„æœŸçš„å†…å®¹
      let hasValidResponse = false;
      switch (provider) {
        case 'openai':
        case 'custom':
          hasValidResponse = !!(data.choices?.[0]?.message?.content);
          break;
        case 'anthropic':
          hasValidResponse = !!(data.content?.[0]?.text);
          break;
        case 'gemini':
          hasValidResponse = !!(data?.candidates?.[0]?.content?.parts?.[0]?.text);
          break;
        default:
          hasValidResponse = !!(data.choices?.[0]?.message?.content);
          break;
      }

      return hasValidResponse;
    } catch (error) {
      console.error('è¿æ¥æµ‹è¯•å¤±è´¥:', error);
      return false;
    }
  }

  // ä¼˜åŒ–æç¤ºè¯ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
  public async optimizePrompt(
    request: AIOptimizeRequest, 
    streamCallback?: StreamCallback
  ): Promise<AIOptimizeResponse> {
    console.log('ğŸ¯ å¼€å§‹AIä¼˜åŒ–è¯·æ±‚', { request, hasCallback: !!streamCallback });
    
    if (!this.isConfigured()) {
      console.log('âŒ AIæœåŠ¡æœªé…ç½®');
      throw new Error('AIæœåŠ¡æœªé…ç½®ï¼Œè¯·å…ˆé…ç½®APIå¯†é’¥');
    }

    const { content, title = '', mode } = request;

    // æ„å»ºç²¾ç®€æç¤ºè¯
    const prompt = PROMPT_OPTIMIZATION_TEMPLATE
      .replace('{title}', title)
      .replace('{content}', content || 'è¯·ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„æç¤ºè¯');

    console.log('ğŸ“ æ„å»ºçš„æç¤ºè¯:', prompt.substring(0, 200) + '...');

    try {
      const response = streamCallback 
        ? await this.callAIStream(prompt, streamCallback)
        : await this.callAI(prompt);
      console.log('âœ… AIå“åº”è·å–æˆåŠŸ', response.substring(0, 100) + '...');
      return this.parseResponse(response);
    } catch (error) {
      console.error('âŒ AIä¼˜åŒ–å¤±è´¥:', error);
      throw new Error('AIæœåŠ¡è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®');
    }
  }

  // è°ƒç”¨AIæœåŠ¡ï¼ˆæµå¼ï¼‰
  private async callAIStream(prompt: string, callback: StreamCallback): Promise<string> {
    if (!this.config) {
      throw new Error('AIé…ç½®æœªæ‰¾åˆ°');
    }

    let fullResponse = '';

    try {
      const { requestBody, headers, requestUrl } = this.buildRequest(prompt, true);

      const response = await fetch(requestUrl, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody)
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`APIè°ƒç”¨å¤±è´¥: ${response.status} ${errorText}`);
      }

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('æ— æ³•è·å–æµå¼å“åº”');
      }

      const decoder = new TextDecoder();
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') continue;
            
            try {
              const parsed = JSON.parse(data);
              const content = this.extractStreamContent(parsed, this.config.provider);
              if (content) {
                fullResponse += content;
                callback.onChunk(content);
              }
            } catch (e) {
              // å¿½ç•¥è§£æé”™è¯¯çš„è¡Œ
            }
          }
        }
      }

      callback.onComplete(fullResponse);
      return fullResponse;
    } catch (error) {
      callback.onError(error as Error);
      throw error;
    }
  }

  // æå–æµå¼å†…å®¹
  private extractStreamContent(data: any, provider: string): string {
    switch (provider) {
      case 'openai':
      case 'custom':
        return data.choices?.[0]?.delta?.content || '';
      case 'anthropic':
        return data.delta?.text || '';
      case 'gemini':
        return data?.candidates?.[0]?.delta?.parts?.[0]?.text || '';
      default:
        return data.choices?.[0]?.delta?.content || '';
    }
  }

  // æ„å»ºè¯·æ±‚å‚æ•°
  private buildRequest(prompt: string, stream: boolean = false) {
    if (!this.config) {
      throw new Error('AIé…ç½®æœªæ‰¾åˆ°');
    }

    const { apiKey, baseUrl, model, provider } = this.config;
    let requestBody: any;
    let headers: Record<string, string>;

    switch (provider) {
      case 'openai':
        requestBody = {
          model: model || 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: prompt }],
          temperature: 0.7,
          max_tokens: 1500,
          stream
        };
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        break;

      case 'anthropic':
        requestBody = {
          model: model || 'claude-3-sonnet-20240229',
          max_tokens: 1500,
          messages: [{ role: 'user', content: prompt }],
          stream
        };
        headers = {
          'Content-Type': 'application/json',
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01'
        };
        break;

      case 'gemini':
        requestBody = {
          contents: [{ role: 'user', parts: [{ text: prompt }] }],
          generationConfig: {
            maxOutputTokens: 1500,
            temperature: 0.7
          }
        };
        headers = { 'Content-Type': 'application/json' };
        break;

      case 'custom':
      default:
        requestBody = {
          model: model,
          messages: [{ role: 'user', content: prompt }],
          temperature: 0.7,
          max_tokens: 1500,
          stream
        };
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        break;
    }

    const requestUrl = this.buildRequestUrl(provider, baseUrl, model, apiKey);
    return { requestBody, headers, requestUrl };
  }

  // æ„å»ºè¯·æ±‚URL
  private buildRequestUrl(provider: string, baseUrl: string, model: string, apiKey: string): string {
    switch (provider) {
      case 'gemini':
        if (baseUrl === 'https://generativelanguage.googleapis.com' || 
            baseUrl === 'https://generativelanguage.googleapis.com/') {
          return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;
        }
        return baseUrl;
        
      case 'openai':
        if (baseUrl === 'https://api.openai.com' || baseUrl === 'https://api.openai.com/') {
          return 'https://api.openai.com/v1/chat/completions';
        }
        if (!baseUrl.includes('/chat/completions')) {
          return baseUrl.replace(/\/$/, '') + '/v1/chat/completions';
        }
        return baseUrl;
        
      case 'anthropic':
        if (baseUrl === 'https://api.anthropic.com' || baseUrl === 'https://api.anthropic.com/') {
          return 'https://api.anthropic.com/v1/messages';
        }
        if (!baseUrl.includes('/messages')) {
          return baseUrl.replace(/\/$/, '') + '/v1/messages';
        }
        return baseUrl;
        
      case 'custom':
      default:
        if (!baseUrl.includes('/chat/completions') && !baseUrl.includes('/v1/')) {
          return baseUrl.replace(/\/$/, '') + '/v1/chat/completions';
        }
        return baseUrl;
    }
  }

  // è°ƒç”¨AIæœåŠ¡ï¼ˆéæµå¼ï¼‰
  private async callAI(prompt: string): Promise<string> {
    if (!this.config) {
      throw new Error('AIé…ç½®æœªæ‰¾åˆ°');
    }

    const { requestBody, headers, requestUrl } = this.buildRequest(prompt, false);

    const response = await fetch(requestUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`APIè°ƒç”¨å¤±è´¥: ${response.status} ${errorText}`);
    }

    const data = await response.json();
    
    // æ ¹æ®ä¸åŒæä¾›å•†è§£æå“åº”
    switch (this.config.provider) {
      case 'openai':
      case 'custom':
        return data.choices?.[0]?.message?.content || '';
      case 'anthropic':
        return data.content?.[0]?.text || '';
      case 'gemini':
        return data?.candidates?.[0]?.content?.parts?.[0]?.text || '';
      default:
        return data.choices?.[0]?.message?.content || '';
    }
  }

  // è§£æAIå“åº”
  public parseResponse(response: string): AIOptimizeResponse {
    try {
      // å°è¯•è§£æç»“æ„åŒ–å“åº”
      const sections = response.split('##');
      
      let optimizedContent = '';
      let explanation = '';
      let suggestions: string[] = [];

      sections.forEach(section => {
        const trimmed = section.trim();
        if (trimmed.startsWith('ä¼˜åŒ–åçš„æç¤ºè¯')) {
          optimizedContent = trimmed.replace('ä¼˜åŒ–åçš„æç¤ºè¯', '').trim();
        } else if (trimmed.startsWith('ä¼˜åŒ–è¯´æ˜')) {
          explanation = trimmed.replace('ä¼˜åŒ–è¯´æ˜', '').trim();
        } else if (trimmed.startsWith('å»ºè®®')) {
          const suggestionText = trimmed.replace('å»ºè®®', '').trim();
          suggestions = suggestionText.split('\n').filter(s => s.trim());
        }
      });

      // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–å†…å®¹ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”ä½œä¸ºä¼˜åŒ–å†…å®¹
      if (!optimizedContent) {
        optimizedContent = response;
      }

      return {
        optimizedContent,
        explanation,
        suggestions
      };
    } catch (error) {
      console.error('è§£æAIå“åº”å¤±è´¥:', error);
      return {
        optimizedContent: response,
        explanation: 'è§£æå“åº”æ—¶å‡ºç°é—®é¢˜ï¼Œä½†å·²è·å¾—ä¼˜åŒ–ç»“æœ',
        suggestions: []
      };
    }
  }
}

// å…¨å±€AIæœåŠ¡å®ä¾‹
export const aiService = new AIService();
